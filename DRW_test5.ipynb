{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPrCllTZXPQQTCj40dNU2Hi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k1W9XJm2qTCo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.stats import pearsonr, zscore\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
        "from scipy.spatial.distance import squareform\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRNppT45q02T",
        "outputId": "2e3d4229-ecd5-4dcd-b594-64a289b29748"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, data_path, file_name):\n",
        "    self.data_path = data_path\n",
        "    self.file_name = file_name\n",
        "\n",
        "  def load(self):\n",
        "    df = pd.read_parquet(self.data_path + self.file_name)\n",
        "    df = df.loc[:, ~((df == -np.inf).any() | (df == 0).all())]\n",
        "    return df"
      ],
      "metadata": {
        "id": "_azN88h-q4ew"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineer:\n",
        "\n",
        "  def __init__(self, df, epsilon=1e-6):\n",
        "    self.df = df\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def develop_features_from_qty_volume(self):\n",
        "    '''\n",
        "      Function to develop features from bid_qty, ask_qty, buy_qty, sell_qty\n",
        "      and volume columns.\n",
        "    '''\n",
        "    imbalance = (self.df['bid_qty'] - self.df['ask_qty']) / (self.df['bid_qty'] + self.df['ask_qty'] + self.epsilon)\n",
        "    buy_sell_ratio = np.log1p(self.df['buy_qty'] / (self.df['sell_qty'] + self.epsilon))\n",
        "    volume_z = zscore(self.df['volume'])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'imbalance': imbalance,\n",
        "        'buy_sell_ratio': buy_sell_ratio,\n",
        "        'volume_z': volume_z\n",
        "    }, index=self.df.index)\n",
        "\n",
        "  def standardize_columns(self, columns):\n",
        "    '''Function to standardize specific columns wihtin a df.'''\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(self.df[columns])\n",
        "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=self.df.index)\n",
        "    return pd.concat([df_scaled, self.df['label']], axis=1)\n",
        "\n",
        "  def standardize_columns_test(self, columns):\n",
        "    '''Function to standardize specific columns wihtin a test df.'''\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(self.df[columns])\n",
        "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=self.df.index)\n",
        "    return df_scaled\n",
        "\n",
        "  def standardize_df(self):\n",
        "    '''Function to standardize a whole df.'''\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(self.df)\n",
        "    return pd.DataFrame(df_scaled, columns=self.df.columns, index=self.df.index)"
      ],
      "metadata": {
        "id": "9D41DP_Pq6ug"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCAProcessor:\n",
        "  def __init__(self, X_selected, groups, path_to_save_results, file_name):\n",
        "    self.X_selected = X_selected\n",
        "    self.groups = groups\n",
        "    self.path_to_save_results = path_to_save_results\n",
        "    self.file_name = file_name\n",
        "\n",
        "  def _pca_transform(self, g, n_components):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    g_pca = pca.fit_transform(self.X_selected[g])\n",
        "    return g_pca\n",
        "\n",
        "  def _pca_explained_variance(self, g):\n",
        "    pca = PCA()\n",
        "    pca.fit(self.X_selected[g])\n",
        "    return pca.explained_variance_ratio_\n",
        "\n",
        "  def _penalized_score(self, g, alpha = 0.005):\n",
        "    scores = []\n",
        "    explained_variance = self._pca_explained_variance(g)\n",
        "\n",
        "    for k in range(1, len(explained_variance) + 1):\n",
        "      score = explained_variance[:k].sum() - alpha * k\n",
        "      scores.append(score)\n",
        "\n",
        "    return np.argmax(scores) + 1, g\n",
        "\n",
        "  def _best_number_of_components(self, alpha = 0.005):\n",
        "    '''Finds the best number of components for each group utilizing the penalized score'''\n",
        "    results = Parallel(n_jobs=-1)(\n",
        "        delayed(self._penalized_score)(g, alpha) for g in self.groups\n",
        "    )\n",
        "\n",
        "    self.n_components = []\n",
        "    self.groups_features = []\n",
        "\n",
        "    for n, g in results:\n",
        "      self.n_components.append(n)\n",
        "      self.groups_features.append(g)\n",
        "    return self.n_components, self.groups_features\n",
        "\n",
        "  def transform_data(self, penalizing_term_alpha = 0.005):\n",
        "    '''A function that combines all the previous methods in order to output a\n",
        "    df that contains all the pca components\n",
        "    '''\n",
        "    self.n_components, self.groups_features = self._best_number_of_components(penalizing_term_alpha)\n",
        "\n",
        "    list_with_all_pca = []\n",
        "    for idx, (g, n) in enumerate(zip(self.groups_features, self.n_components), start=1):\n",
        "      g_pca = self._pca_transform(g, n)\n",
        "\n",
        "      col_names = [f\"G{idx}P{j+1}\" for j in range(n)]\n",
        "      df_pca_group = pd.DataFrame(g_pca, columns=col_names, index=self.X_selected.index)\n",
        "\n",
        "      list_with_all_pca.append(df_pca_group)\n",
        "\n",
        "    # Save the data to JSON\n",
        "    self.n_components = [int(x) for x in self.n_components]\n",
        "    best_PCA = [self.n_components, self.groups_features]\n",
        "    with open(self.path_to_save_results + self.file_name, \"w\") as f:\n",
        "        json.dump(best_PCA, f, indent=2)\n",
        "\n",
        "    return pd.concat(list_with_all_pca, axis=1)\n",
        "\n",
        "  def transform_data_test(self, n_components):\n",
        "    list_with_all_pca = []\n",
        "    for idx, (g, n) in enumerate(zip(self.groups, n_components), start=1):\n",
        "      g_pca = self._pca_transform(g, n)\n",
        "\n",
        "      col_names = [f\"G{idx}P{j+1}\" for j in range(n)]\n",
        "      df_pca_group = pd.DataFrame(g_pca, columns=col_names, index=self.X_selected.index)\n",
        "\n",
        "      list_with_all_pca.append(df_pca_group)\n",
        "\n",
        "    return pd.concat(list_with_all_pca, axis=1)\n",
        ""
      ],
      "metadata": {
        "id": "71rn9tkcrFQH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LagCreator:\n",
        "  def __init__(self, lag_periods = 5):\n",
        "    self.lag_periods = lag_periods\n",
        "\n",
        "  def lag_features(self, X):\n",
        "\n",
        "    lagged_columns = []\n",
        "    for col in X.columns:\n",
        "        for lag in range(1, self.lag_periods + 1):\n",
        "          shifted = X[col].shift(lag)\n",
        "          shifted.name = f\"{col}_lag{lag}\"\n",
        "          lagged_columns.append(shifted)\n",
        "\n",
        "    lagged = pd.concat(lagged_columns, axis=1)\n",
        "    return lagged.dropna()\n",
        "\n",
        "  def match_index_lags(self, X_lagged, Y):\n",
        "    '''Y is series with a single column'''\n",
        "    y_lagged = Y.loc[X_lagged.index]\n",
        "\n",
        "    return y_lagged\n"
      ],
      "metadata": {
        "id": "8KzH9pXdrF64"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadGroups:\n",
        "  def __init__(self, data_path, file_name):\n",
        "    self.data_path = data_path\n",
        "    self.file_name = file_name\n",
        "\n",
        "  def _load_json(self):\n",
        "    with open(self.data_path + self.file_name, 'r') as f:\n",
        "      groups = json.load(f)\n",
        "    return groups\n",
        "\n",
        "  def create_components_groups_split(self):\n",
        "    groups = self._load_json()\n",
        "    self.g = groups[1]\n",
        "    return groups[0], self.g\n",
        "\n",
        "  def flatten_groups(self):\n",
        "    return [item for sublist in self.g for item in sublist]"
      ],
      "metadata": {
        "id": "Nt6nNiy7rWpZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelPredictor:\n",
        "  def __init__(self, model_name, submission_name, path):\n",
        "    self.path = path\n",
        "    self.model_name = model_name\n",
        "    self.submission_name = submission_name\n",
        "\n",
        "  def _load_model(self):\n",
        "    model = joblib.load(self.path + self.model_name)\n",
        "    return model\n",
        "\n",
        "  def predict(self, X):\n",
        "    self.model = self._load_model()\n",
        "    self.y = self.model.predict(X)\n",
        "    return self.y\n",
        "\n",
        "  def save_results(self):\n",
        "    zero_part = np.zeros(5)\n",
        "    combined = np.concatenate((zero_part, self.y))\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "    'ID': np.arange(1, len(combined) + 1),\n",
        "    'prediction': combined\n",
        "    })\n",
        "\n",
        "    submission.to_csv(self.path + self.submission_name, index=False)\n",
        "\n",
        "    return submission"
      ],
      "metadata": {
        "id": "KIRol7bA0uo6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filepaths\n",
        "dataPath = '/content/drive/MyDrive/Colab Notebooks/DRW/data/'\n",
        "fileNameTest = 'test.parquet'\n",
        "fileNameGroups = 'best_features_5_1.json'\n",
        "modelName = 'final_xgboost_model_5_1.pkl'"
      ],
      "metadata": {
        "id": "E-qWKU3lrLT_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "dataLoader = DataLoader(dataPath, fileNameTest)\n",
        "df_test = dataLoader.load()"
      ],
      "metadata": {
        "id": "I0GGggjjssRi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the number of components and the features for each PCA\n",
        "groupLoader = LoadGroups(dataPath, fileNameGroups)\n",
        "components, groups = groupLoader.create_components_groups_split()\n",
        "flat_groups = groupLoader.flatten_groups()"
      ],
      "metadata": {
        "id": "1CNLB5B2sSWe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features creation\n",
        "featureEngineer = FeatureEngineer(df_test)\n",
        "df_qty_vol = featureEngineer.develop_features_from_qty_volume()\n",
        "standardized_df_qty_vol = FeatureEngineer(df_qty_vol).standardize_df()\n",
        "df_scaled_X_features = featureEngineer.standardize_columns_test(flat_groups)"
      ],
      "metadata": {
        "id": "012cYCk_sWUp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA features by groups\n",
        "PCAProc = PCAProcessor(df_scaled_X_features, groups, '0', '0')\n",
        "X_pca = PCAProc.transform_data_test(components)"
      ],
      "metadata": {
        "id": "YjLRHiI8zQbU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the X _test\n",
        "X = pd.concat([X_pca, standardized_df_qty_vol], axis=1)"
      ],
      "metadata": {
        "id": "0tuHXEBEtJdc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lag the features with lag 5\n",
        "lagger = LagCreator()\n",
        "X_lagged = lagger.lag_features(X)"
      ],
      "metadata": {
        "id": "Bpk8-pIhwBV6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prediciton and build a submission\n",
        "prediction = ModelPredictor(modelName, 'submission7.csv', dataPath)\n",
        "\n",
        "y = prediction.predict(X_lagged)\n",
        "subm = prediction.save_results()"
      ],
      "metadata": {
        "id": "OOeopo_jwBMF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(subm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoGdpjvl25Ud",
        "outputId": "e2a0e24a-dfbe-420c-b226-d0667132c8d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            ID  prediction\n",
            "0            1    0.000000\n",
            "1            2    0.000000\n",
            "2            3    0.000000\n",
            "3            4    0.000000\n",
            "4            5    0.000000\n",
            "...        ...         ...\n",
            "538145  538146   -0.012840\n",
            "538146  538147    0.037183\n",
            "538147  538148    0.072205\n",
            "538148  538149    0.021216\n",
            "538149  538150   -0.003651\n",
            "\n",
            "[538150 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}