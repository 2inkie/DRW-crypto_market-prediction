{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPiKrS34WeIzMAR/B1HIkgu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k2ADiJr3XuR",
        "outputId": "149097fb-3db4-4536-a7bb-c400c8917c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: kneed in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from kneed) (1.15.3)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install optuna kneed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, json, pickle, warnings, optuna, joblib\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from kneed import KneeLocator\n",
        "from google.colab import drive\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import Lasso\n",
        "from collections import defaultdict\n",
        "from scipy.stats import pearsonr, zscore\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram"
      ],
      "metadata": {
        "id": "jd5E9YwN3kG9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmEt2lm33uw4",
        "outputId": "086ac01c-ea73-4cea-92a4-b39b2fa6192c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, data_path, file_name):\n",
        "    self.data_path = data_path\n",
        "    self.file_name = file_name\n",
        "\n",
        "  def load(self):\n",
        "    df = pd.read_parquet(self.data_path + self.file_name)\n",
        "    self.df = df.loc[:, ~((df == -np.inf).any() | (df == 0).all())]\n",
        "    print('Data loaded')\n",
        "    return self\n",
        "\n",
        "  def train_split(self):\n",
        "    label = self.df['label']\n",
        "    qty_vol = self.df[['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']]\n",
        "    X_ = self.df.drop(['label', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume'], axis=1)\n",
        "    return X_, qty_vol, label\n",
        "\n",
        "  def test_split(self):\n",
        "    qty_vol = self.df[['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']]\n",
        "    X_ = self.df.drop(['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume'], axis=1)\n",
        "    return X_, qty_vol\n",
        "\n",
        "  def load_features_json_list(self):\n",
        "    with open(self.data_path + self.file_name, 'r') as f:\n",
        "      return json.load(f)\n",
        "\n",
        "  def load_clusters_pkl(self):\n",
        "    with open(self.data_path + self.file_name, 'rb') as f:\n",
        "      return pickle.load(f)"
      ],
      "metadata": {
        "id": "JNdJcZnS3zCN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureGenerator:\n",
        "  def __init__(self, df, epsilon=1e-6):\n",
        "    self.df = df\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def develop_features_from_qty_volume(self):\n",
        "    '''\n",
        "      Function to develop features from bid_qty, ask_qty, buy_qty, sell_qty\n",
        "      and volume columns.\n",
        "    '''\n",
        "    imbalance = (self.df['bid_qty'] - self.df['ask_qty']) / (self.df['bid_qty'] + self.df['ask_qty'] + self.epsilon)\n",
        "    buy_sell_ratio = np.log1p(self.df['buy_qty'] / (self.df['sell_qty'] + self.epsilon))\n",
        "    volume_z = zscore(self.df['volume'])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'imbalance': imbalance,\n",
        "        'buy_sell_ratio': buy_sell_ratio,\n",
        "        'volume_z': volume_z\n",
        "        }, index=self.df.index)\n",
        "\n",
        "  def standardize_columns(self, columns):\n",
        "    '''Function to standardize specific columns wihtin a df.'''\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(self.df[columns])\n",
        "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=self.df.index)\n",
        "    return pd.concat([df_scaled, self.df['label']], axis=1)\n",
        "\n",
        "  def standardize_df(self):\n",
        "    '''Function to standardize a whole df.'''\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(self.df)\n",
        "    return pd.DataFrame(df_scaled, columns=self.df.columns, index=self.df.index)"
      ],
      "metadata": {
        "id": "1DbSBDeV31M7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LagCreator:\n",
        "  def __init__(self, lag_periods = 5):\n",
        "    self.lag_periods = lag_periods\n",
        "\n",
        "  def lag_features(self, X):\n",
        "\n",
        "    lagged_columns = []\n",
        "    for col in X.columns:\n",
        "        for lag in range(1, self.lag_periods + 1):\n",
        "          shifted = X[col].shift(lag)\n",
        "          shifted.name = f\"{col}_lag{lag}\"\n",
        "          lagged_columns.append(shifted)\n",
        "\n",
        "    lagged = pd.concat(lagged_columns, axis=1)\n",
        "    lagged = pd.concat([X, lagged], axis=1)\n",
        "    return lagged.dropna()\n",
        "\n",
        "  def match_index_lags(self, X_lagged, Y):\n",
        "    '''Y is series with a single column'''\n",
        "    y_lagged = Y.loc[X_lagged.index]\n",
        "\n",
        "    return y_lagged\n"
      ],
      "metadata": {
        "id": "ENa6nyt3399B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureLagSelector:\n",
        "    def __init__(self, feature_list):\n",
        "        '''\n",
        "        Initialize the selector with a list of desired features.\n",
        "        Features can be raw (e.g., 'X264') or lagged (e.g., 'X264_lag2')\n",
        "        '''\n",
        "        self.feature_list = feature_list\n",
        "        self.selected_features = {}\n",
        "\n",
        "    def _parse_feature(self, feature_name):\n",
        "\n",
        "        '''Parse feature name into base column and lag value (if present).'''\n",
        "\n",
        "        if '_lag' in feature_name:\n",
        "            base, lag = feature_name.split('_lag')\n",
        "            return base, int(lag)\n",
        "        return feature_name, None\n",
        "\n",
        "    def _transform(self, df):\n",
        "        '''Build a new DataFrame with the selected and lagged features.'''\n",
        "\n",
        "        self.selected_features.clear()\n",
        "        for feature in self.feature_list:\n",
        "            base, lag = self._parse_feature(feature)\n",
        "            if base not in df.columns:\n",
        "                print(f\"Base column '{base}' not found in DataFrame\")\n",
        "                continue\n",
        "            if lag is None:\n",
        "                self.selected_features[feature] = df[base]\n",
        "            else:\n",
        "                self.selected_features[feature] = df[base].shift(lag)\n",
        "\n",
        "        return pd.DataFrame(self.selected_features)\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        return self._transform(df)\n"
      ],
      "metadata": {
        "id": "FLM3rb8t3-0j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelPipelineOptimizerLasso:\n",
        "  def __init__ (self, y, df, features2, clusters_components, model_save_location, model_name):\n",
        "    self.df = df\n",
        "    self.features2 = features2\n",
        "    self.y = y\n",
        "    self.clusters_components = clusters_components\n",
        "    self.data_path = model_save_location\n",
        "    self.model_name = model_name\n",
        "\n",
        "    self.best_features = [f for group in self.clusters_components[0] for f in group]\n",
        "    self.X = FeatureLagSelector(self.best_features).fit_transform(self.df).dropna()\n",
        "    self.features2 = LagCreator(3).lag_features(self.features2)\n",
        "\n",
        "    self.X = pd.concat([self.X, self.features2], axis=1)\n",
        "\n",
        "    self.y = self.y.loc[self.X.index]\n",
        "\n",
        "\n",
        "    del self.df, self.features2\n",
        "    gc.collect()\n",
        "\n",
        "  def create_column_transformer(self):\n",
        "    feature_clusters, pca_components = self.clusters_components\n",
        "    transformers = []\n",
        "\n",
        "    for i, (features, n_comp) in enumerate(zip(feature_clusters, pca_components)):\n",
        "      if len(features) == 1:\n",
        "            # For single feature, just apply StandardScaler\n",
        "            pipe = Pipeline([\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "      else:\n",
        "            # Multiple features: apply scaler + PCA\n",
        "            pipe = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('pca', PCA(n_components=n_comp))\n",
        "            ])\n",
        "      transformers.append((f'cluster_{i}', pipe, features))\n",
        "\n",
        "    return ColumnTransformer(transformers, remainder='passthrough')\n",
        "\n",
        "  def objective(self, trial):\n",
        "    lasso_params = {\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 10.0, log=True),\n",
        "        \"max_iter\": trial.suggest_int(\"max_iter\", 1000, 10000),\n",
        "        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True),\n",
        "        \"random_state\": 42\n",
        "        }\n",
        "\n",
        "\n",
        "    col_transformer = self.create_column_transformer()\n",
        "    tscv = TimeSeriesSplit(n_splits=10)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in tscv.split(self.X):\n",
        "      X_train, X_val = self.X.iloc[train_idx], self.X.iloc[val_idx]\n",
        "      y_train, y_val = self.y.iloc[train_idx], self.y.iloc[val_idx]\n",
        "\n",
        "      pipeline = Pipeline([\n",
        "          ('pca_clusters', col_transformer),\n",
        "          ('lasso', Lasso(**lasso_params))\n",
        "      ])\n",
        "\n",
        "      pipeline.fit(X_train, y_train)\n",
        "      preds = pipeline.predict(X_val)\n",
        "\n",
        "      corr, _ = pearsonr(y_val, preds)\n",
        "      scores.append(corr)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "  def optimize(self, n_trials=50):\n",
        "    self.study = optuna.create_study(direction=\"maximize\")\n",
        "    self.study.optimize(self.objective, n_trials=n_trials, n_jobs=-1)\n",
        "\n",
        "    print(\"\\nBest trial:\")\n",
        "    print(f\"  Pearson correlation: {self.study.best_value:.4f}\")\n",
        "    print(\"  Hyperparameters:\")\n",
        "    for key, val in self.study.best_params.items():\n",
        "        print(f\"    {key}: {val}\")\n",
        "\n",
        "    # Visualize optimization\n",
        "    optuna.visualization.plot_optimization_history(self.study).show()\n",
        "    optuna.visualization.plot_param_importances(self.study).show()\n",
        "\n",
        "    # Recreate PCA + scaler transformer\n",
        "    col_transformer = self.create_column_transformer()\n",
        "\n",
        "    # Build final pipeline with best Lasso params\n",
        "    best_params = self.study.best_params.copy()\n",
        "    best_params.update({\"random_state\": 42})\n",
        "\n",
        "    final_pipeline = Pipeline([\n",
        "        ('pca_clusters', col_transformer),\n",
        "        ('lasso', Lasso(**best_params))\n",
        "    ])\n",
        "\n",
        "    # Fit final pipeline on all data\n",
        "    final_pipeline.fit(self.X, self.y)\n",
        "\n",
        "    # Save pipeline\n",
        "    joblib.dump(final_pipeline, self.data_path + self.model_name)\n",
        "    print(f\"\\nModel saved to: {self.data_path + self.model_name}\")\n",
        "\n",
        "    return final_pipeline"
      ],
      "metadata": {
        "id": "6R2MAjcD4KKa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features\n",
        "# Group 1 - X1, X2 ....\n",
        "# Group 2 - _qty, vol"
      ],
      "metadata": {
        "id": "Mqqjyg1a5RBj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filepaths\n",
        "dataPathLoad = '/content/drive/MyDrive/Colab Notebooks/DRW/data/'\n",
        "dataPathSave = '/content/drive/MyDrive/Colab Notebooks/DRW/data/v7'\n",
        "fileNameTrain = 'train.parquet'"
      ],
      "metadata": {
        "id": "C3xk7No15Tpo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train data and remove -inf columns and columns full of 0\n",
        "loader = DataLoader(dataPathLoad, fileNameTrain)\n",
        "features_group1, features_group2, label = loader.load().train_split()\n",
        "del loader\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzT8opG85VR6",
        "outputId": "46383100-6cb6-423b-bd99-636af89a509e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the right features2\n",
        "features_group2 = FeatureGenerator(features_group2).develop_features_from_qty_volume()\n"
      ],
      "metadata": {
        "id": "0KCwmQsU5XS7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the groups\n",
        "with open(dataPathSave + 'PCA_components7.json', 'r') as f:\n",
        "    groups_components = json.load(f)"
      ],
      "metadata": {
        "id": "0VTHGRfH5a5J"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize and save model\n",
        "optimizer = ModelPipelineOptimizerLasso(label, features_group1, features_group2, groups_components, dataPathSave, 'model7_10splits_Lasso.pkl')\n",
        "final_pipeline = optimizer.optimize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwH0l4FE5etR",
        "outputId": "c4c4bfb7-0cd1-4440-d6a2-027ac9fbcbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-07 15:02:04,097] A new study created in memory with name: no-name-5f1dddef-8e54-45ac-b0fa-17f6f40e3d9c\n",
            "[I 2025-06-07 15:57:34,776] Trial 3 finished with value: 0.09691958199460751 and parameters: {'alpha': 0.24740989558368395, 'max_iter': 9313, 'tol': 0.00011386502300864879}. Best is trial 3 with value: 0.09691958199460751.\n",
            "[I 2025-06-07 15:57:34,777] Trial 5 finished with value: 0.09676261468968048 and parameters: {'alpha': 0.2555614513684374, 'max_iter': 9446, 'tol': 0.0004096613835651075}. Best is trial 3 with value: 0.09691958199460751.\n",
            "[I 2025-06-07 16:01:19,384] Trial 0 finished with value: 0.09448796027964908 and parameters: {'alpha': 0.13376478253294793, 'max_iter': 1089, 'tol': 3.406032916422772e-05}. Best is trial 3 with value: 0.09691958199460751.\n",
            "[I 2025-06-07 16:06:28,630] Trial 4 finished with value: 0.095226951876572 and parameters: {'alpha': 0.10325678940836908, 'max_iter': 6600, 'tol': 0.00025680251699274797}. Best is trial 3 with value: 0.09691958199460751.\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n",
            "[I 2025-06-07 16:06:28,744] Trial 1 finished with value: 0.09814299303762689 and parameters: {'alpha': 0.07746538343378148, 'max_iter': 5746, 'tol': 0.0001835637734161102}. Best is trial 1 with value: 0.09814299303762689.\n",
            "[I 2025-06-07 16:07:32,849] Trial 7 finished with value: 0.10363769184405813 and parameters: {'alpha': 0.03159107457401706, 'max_iter': 4828, 'tol': 0.00013950468997100718}. Best is trial 7 with value: 0.10363769184405813.\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n",
            "[I 2025-06-07 16:32:16,657] Trial 6 finished with value: 0.0706744464863728 and parameters: {'alpha': 0.0025054973888373024, 'max_iter': 1763, 'tol': 0.000101598953613902}. Best is trial 7 with value: 0.10363769184405813.\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n",
            "<ipython-input-8-5ed8a51dc52c>:67: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(y_val, preds)\n"
          ]
        }
      ]
    }
  ]
}