{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNdAHK/F1PFXGaWTRnK/VVj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmzHivCGuMG-",
        "outputId": "33f9ecec-bcb9-4022-96c8-8896be9c1774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, json, pickle, warnings, optuna, joblib\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from collections import defaultdict\n",
        "from scipy.stats import pearsonr, zscore\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram"
      ],
      "metadata": {
        "id": "Rp54DykEun3K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.deprecation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIy29X8MwCtK",
        "outputId": "3fa9cdb9-823b-49e2-e092-5d7c564990ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, data_path, file_name):\n",
        "    self.data_path = data_path\n",
        "    self.file_name = file_name\n",
        "\n",
        "  def load(self):\n",
        "    df = pd.read_parquet(self.data_path + self.file_name)\n",
        "    self.df = df.loc[:, ~((df == -np.inf).any() | (df == 0).all())]\n",
        "    print('Data loaded')\n",
        "    return self\n",
        "\n",
        "  def train_split(self):\n",
        "    label = self.df['label']\n",
        "    qty_vol = self.df[['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']]\n",
        "    X_ = self.df.drop(['label', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume'], axis=1)\n",
        "    return X_, qty_vol, label\n",
        "\n",
        "  def test_split(self):\n",
        "    qty_vol = self.df[['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']]\n",
        "    X_ = self.df.drop(['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume'], axis=1)\n",
        "    return X_, qty_vol\n",
        "\n",
        "  def load_features_json_list(self):\n",
        "    with open(self.data_path + self.file_name, 'r') as f:\n",
        "      return json.load(f)\n",
        "\n",
        "  def load_clusters_pkl(self):\n",
        "    with open(self.data_path + self.file_name, 'rb') as f:\n",
        "      return pickle.load(f)"
      ],
      "metadata": {
        "id": "e-DRyqAOwJ8w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureGenerator:\n",
        "  def __init__(self, df, epsilon=1e-6):\n",
        "    self.df = df\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def develop_features_from_qty_volume(self):\n",
        "    '''\n",
        "      Function to develop features from bid_qty, ask_qty, buy_qty, sell_qty\n",
        "      and volume columns.\n",
        "    '''\n",
        "    imbalance = (self.df['bid_qty'] - self.df['ask_qty']) / (self.df['bid_qty'] + self.df['ask_qty'] + self.epsilon)\n",
        "    buy_sell_ratio = np.log1p(self.df['buy_qty'] / (self.df['sell_qty'] + self.epsilon))\n",
        "    volume_z = zscore(self.df['volume'])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'imbalance': imbalance,\n",
        "        'buy_sell_ratio': buy_sell_ratio,\n",
        "        'volume_z': volume_z\n",
        "        }, index=self.df.index)\n",
        "\n",
        "  def standardize_columns(self, columns):\n",
        "    '''Function to standardize specific columns wihtin a df.'''\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(self.df[columns])\n",
        "    df_scaled = pd.DataFrame(df_scaled, columns=columns, index=self.df.index)\n",
        "    return pd.concat([df_scaled, self.df['label']], axis=1)\n",
        "\n",
        "  def standardize_df(self):\n",
        "    '''Function to standardize a whole df.'''\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(self.df)\n",
        "    return pd.DataFrame(df_scaled, columns=self.df.columns, index=self.df.index)"
      ],
      "metadata": {
        "id": "CYs_Q80gwN3c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LagCreator:\n",
        "  def __init__(self, lag_periods = 5):\n",
        "    self.lag_periods = lag_periods\n",
        "\n",
        "  def lag_features(self, X):\n",
        "\n",
        "    lagged_columns = []\n",
        "    for col in X.columns:\n",
        "        for lag in range(1, self.lag_periods + 1):\n",
        "          shifted = X[col].shift(lag)\n",
        "          shifted.name = f\"{col}_lag{lag}\"\n",
        "          lagged_columns.append(shifted)\n",
        "\n",
        "    lagged = pd.concat(lagged_columns, axis=1)\n",
        "    lagged = pd.concat([X, lagged], axis=1)\n",
        "    return lagged.dropna()\n",
        "\n",
        "  def match_index_lags(self, X_lagged, Y):\n",
        "    '''Y is series with a single column'''\n",
        "    y_lagged = Y.loc[X_lagged.index]\n",
        "\n",
        "    return y_lagged\n"
      ],
      "metadata": {
        "id": "PBf3r_DiwUkV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureLagSelector:\n",
        "    def __init__(self, feature_list):\n",
        "        '''\n",
        "        Initialize the selector with a list of desired features.\n",
        "        Features can be raw (e.g., 'X264') or lagged (e.g., 'X264_lag2')\n",
        "        '''\n",
        "        self.feature_list = feature_list\n",
        "        self.selected_features = {}\n",
        "\n",
        "    def _parse_feature(self, feature_name):\n",
        "\n",
        "        '''Parse feature name into base column and lag value (if present).'''\n",
        "\n",
        "        if '_lag' in feature_name:\n",
        "            base, lag = feature_name.split('_lag')\n",
        "            return base, int(lag)\n",
        "        return feature_name, None\n",
        "\n",
        "    def _transform(self, df):\n",
        "        '''Build a new DataFrame with the selected and lagged features.'''\n",
        "\n",
        "        self.selected_features.clear()\n",
        "        for feature in self.feature_list:\n",
        "            base, lag = self._parse_feature(feature)\n",
        "            if base not in df.columns:\n",
        "                print(f\"Base column '{base}' not found in DataFrame\")\n",
        "                continue\n",
        "            if lag is None:\n",
        "                self.selected_features[feature] = df[base]\n",
        "            else:\n",
        "                self.selected_features[feature] = df[base].shift(lag)\n",
        "\n",
        "        return pd.DataFrame(self.selected_features)\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        return self._transform(df)\n"
      ],
      "metadata": {
        "id": "WBRFvk31waph"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelPipelineOptimizerNN:\n",
        "  def __init__ (self, y, df, features2, clusters_components, model_save_location, model_name):\n",
        "    self.df = df\n",
        "    self.features2 = features2\n",
        "    self.y = y\n",
        "    self.clusters_components = clusters_components\n",
        "    self.data_path = model_save_location\n",
        "    self.model_name = model_name\n",
        "\n",
        "    self.best_features = [f for group in self.clusters_components[0] for f in group]\n",
        "    self.X = FeatureLagSelector(self.best_features).fit_transform(self.df).dropna()\n",
        "    self.features2 = LagCreator(3).lag_features(self.features2)\n",
        "\n",
        "    self.X = pd.concat([self.X, self.features2], axis=1)\n",
        "\n",
        "    self.y = self.y.loc[self.X.index]\n",
        "\n",
        "\n",
        "    del self.df, self.features2\n",
        "    gc.collect()\n",
        "\n",
        "  def create_column_transformer(self):\n",
        "    feature_clusters, pca_components = self.clusters_components\n",
        "    transformers = []\n",
        "\n",
        "    for i, (features, n_comp) in enumerate(zip(feature_clusters, pca_components)):\n",
        "      if len(features) == 1:\n",
        "            # For single feature, just apply StandardScaler\n",
        "            pipe = Pipeline([\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "      else:\n",
        "            # Multiple features: apply scaler + PCA\n",
        "            pipe = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('pca', PCA(n_components=n_comp))\n",
        "            ])\n",
        "      transformers.append((f'cluster_{i}', pipe, features))\n",
        "\n",
        "    return ColumnTransformer(transformers, remainder='passthrough')\n",
        "\n",
        "  def build_model(self, input_dim, trial):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    model.add(Dense(trial.suggest_int('units1', 128, 384), activation='relu'))\n",
        "    model.add(Dropout(trial.suggest_float('dropout1', 0.2, 0.4)))\n",
        "\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(\n",
        "        loss='mse',\n",
        "        optimizer=Adam(\n",
        "            learning_rate=trial.suggest_float('lr', 0.002, 0.008, log=True)\n",
        "        )\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "  def objective(self, trial):\n",
        "    col_transformer = self.create_column_transformer()\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in tscv.split(self.X):\n",
        "      X_train, X_val = self.X.iloc[train_idx], self.X.iloc[val_idx]\n",
        "      y_train, y_val = self.y.iloc[train_idx], self.y.iloc[val_idx]\n",
        "\n",
        "      Xt_train = col_transformer.fit_transform(X_train)\n",
        "      Xt_val = col_transformer.transform(X_val)\n",
        "\n",
        "      model = self.build_model(Xt_train.shape[1], trial)\n",
        "      model.fit(\n",
        "          Xt_train,\n",
        "          y_train,\n",
        "          epochs=trial.suggest_int(\"epochs\", 50, 100),\n",
        "          batch_size=trial.suggest_categorical(\"batch_size\", [64, 128, 256]),\n",
        "          verbose=0)\n",
        "\n",
        "      preds = model.predict(Xt_val).flatten()\n",
        "      corr, _ = pearsonr(y_val, preds)\n",
        "      scores.append(pearsonr(y_val, preds.flatten())[0])\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "  def optimize(self, n_trials=50):\n",
        "    self.study = optuna.create_study(direction=\"maximize\")\n",
        "    self.study.optimize(self.objective, n_trials=n_trials, n_jobs=-1)\n",
        "\n",
        "    print(\"\\nBest trial:\")\n",
        "    print(f\"  Pearson correlation: {self.study.best_value:.4f}\")\n",
        "    print(\"  Hyperparameters:\")\n",
        "    for key, val in self.study.best_params.items():\n",
        "        print(f\"    {key}: {val}\")\n",
        "\n",
        "    # Visualize optimization\n",
        "    optuna.visualization.plot_optimization_history(self.study).show()\n",
        "    optuna.visualization.plot_param_importances(self.study).show()\n",
        "\n",
        "    # Recreate PCA + scaler transformer\n",
        "    col_transformer = self.create_column_transformer()\n",
        "    Xt = col_transformer.fit_transform(self.X)\n",
        "\n",
        "    # Build final pipeline with best NN params\n",
        "    best_trial = self.study.best_trial\n",
        "    final_model = self.build_model(Xt.shape[1], best_trial)\n",
        "    final_model.fit(Xt, self.y, epochs=best_trial.params['epochs'], batch_size=best_trial.params['batch_size'], verbose=0)\n",
        "\n",
        "    joblib.dump((col_transformer, final_model), self.data_path + self.model_name)\n",
        "    print(f\"\\nModel saved to: {self.data_path + self.model_name}\")\n",
        "\n",
        "    return final_model"
      ],
      "metadata": {
        "id": "u3tPwbx6whLI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features\n",
        "# Group 1 - X1, X2 ....\n",
        "# Group 2 - _qty, vol"
      ],
      "metadata": {
        "id": "mmM3Ejm30IJq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filepaths\n",
        "dataPathLoad = '/content/drive/MyDrive/Colab Notebooks/DRW/data/'\n",
        "dataPathSave = '/content/drive/MyDrive/Colab Notebooks/DRW/data/v7'\n",
        "fileNameTrain = 'train.parquet'"
      ],
      "metadata": {
        "id": "g13OZkhV0Jzw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train data and remove -inf columns and columns full of 0\n",
        "loader = DataLoader(dataPathLoad, fileNameTrain)\n",
        "features_group1, features_group2, label = loader.load().train_split()\n",
        "del loader\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpMJ_8k70LKb",
        "outputId": "88ac1590-cd98-45c0-ca90-15fef37a9352"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the right features2\n",
        "features_group2 = FeatureGenerator(features_group2).develop_features_from_qty_volume()\n"
      ],
      "metadata": {
        "id": "wPVGP50U0NS_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the groups\n",
        "with open(dataPathSave + 'PCA_components7.json', 'r') as f:\n",
        "    groups_components = json.load(f)"
      ],
      "metadata": {
        "id": "CFCSk03T0O2N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize and save model\n",
        "model_name = 'model7_5splits_NN_2.pkl'\n",
        "optimizer = ModelPipelineOptimizerNN(label, features_group1, features_group2, groups_components, dataPathSave, model_name)\n",
        "final_pipeline = optimizer.optimize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiaq34Tz0QZi",
        "outputId": "0d2d6e79-464a-4330-a1f6-05ce0ceb26e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-09 13:29:58,374] A new study created in memory with name: no-name-43678368-1373-49df-ac34-dabc7d5bea3b\n"
          ]
        }
      ]
    }
  ]
}